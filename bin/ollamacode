#!/bin/bash
#
# ollamaCode - Interactive CLI for Ollama (Claude Code style)
# Version: 1.0.0
# Author: Core.at
# License: MIT
#

set -euo pipefail

# Configuration
VERSION="1.0.0"
CONFIG_DIR="${HOME}/.config/ollamacode"
CONFIG_FILE="${CONFIG_DIR}/config"
HISTORY_FILE="${CONFIG_DIR}/history"
SESSION_DIR="${CONFIG_DIR}/sessions"

# Get script directory for loading libraries
# Try development location first, then installed location
if [[ -d "$(cd "$(dirname "${BASH_SOURCE[0]}")/.." 2>/dev/null && pwd)/lib" ]]; then
    # Development mode: libs are in ../lib relative to bin/
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/../lib" && pwd)"
elif [[ -d "/usr/local/lib/ollamacode" ]]; then
    # Installed mode: libs are in /usr/local/lib/ollamacode
    SCRIPT_DIR="/usr/local/lib/ollamacode"
else
    echo "Error: Could not find ollamacode library files" >&2
    echo "Searched:" >&2
    echo "  - $(dirname "${BASH_SOURCE[0]}")/../lib" >&2
    echo "  - /usr/local/lib/ollamacode" >&2
    exit 1
fi

# Source tool libraries
source "${SCRIPT_DIR}/system_prompt.sh"
source "${SCRIPT_DIR}/tool_parser.sh"
source "${SCRIPT_DIR}/tool_executor.sh"

# Default settings
DEFAULT_MODEL="coreEchoFlux"
DEFAULT_OLLAMA_HOST="http://localhost:11434"
DEFAULT_TEMPERATURE="0.7"
DEFAULT_MAX_TOKENS="2048"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color
BOLD='\033[1m'

# Initialize configuration
init_config() {
    mkdir -p "${CONFIG_DIR}" "${SESSION_DIR}"

    if [[ ! -f "${CONFIG_FILE}" ]]; then
        cat > "${CONFIG_FILE}" << EOF
# ollamaCode Configuration
MODEL=${DEFAULT_MODEL}
OLLAMA_HOST=${DEFAULT_OLLAMA_HOST}
TEMPERATURE=${DEFAULT_TEMPERATURE}
MAX_TOKENS=${DEFAULT_MAX_TOKENS}
SYSTEM_PROMPT="You are a helpful AI coding assistant."
EOF
        echo -e "${GREEN}✓ Configuration created at ${CONFIG_FILE}${NC}"
    fi

    touch "${HISTORY_FILE}"
}

# Load configuration
load_config() {
    if [[ -f "${CONFIG_FILE}" ]]; then
        # shellcheck disable=SC1090
        source "${CONFIG_FILE}"
    fi
}

# Print banner
print_banner() {
    echo -e "${CYAN}${BOLD}"
    cat << 'EOF'
   ____  _ _                       ____          _
  / __ \| | | __ _ _ __ ___   __ _/ ___|___   __| | ___
 | |  | | | |/ _` | '_ ` _ \ / _` | |   / _ \ / _` |/ _ \
 | |__| | | | (_| | | | | | | (_| | |__| (_) | (_| |  __/
  \____/|_|_|\__,_|_| |_| |_|\__,_|\____\___/ \__,_|\___|

EOF
    echo -e "${NC}${BLUE}Interactive CLI for Ollama - Version ${VERSION}${NC}"
    echo -e "${YELLOW}Type 'help' for commands, 'exit' to quit${NC}"
    echo ""
}

# Usage information
usage() {
    cat << EOF
Usage: ollamacode [OPTIONS] [PROMPT]

Interactive CLI for Ollama with Claude Code-like features

OPTIONS:
    -m, --model MODEL       Use specific model (default: ${MODEL})
    -t, --temperature NUM   Set temperature (default: ${TEMPERATURE})
    -s, --system PROMPT     Set system prompt
    -f, --file FILE         Read prompt from file
    -c, --config            Show current configuration
    -l, --list              List available models
    -v, --version           Show version
    -h, --help              Show this help

INTERACTIVE COMMANDS:
    help                    Show available commands
    models                  List available models
    use MODEL               Switch to different model
    temp NUM                Set temperature
    system PROMPT           Set system prompt
    history                 Show conversation history
    save [NAME]             Save current session
    load NAME               Load saved session
    clear                   Clear screen
    config                  Show configuration
    exit, quit              Exit ollamacode

EXAMPLES:
    ollamacode                              # Start interactive mode
    ollamacode "Explain Docker"             # Single prompt
    ollamacode -m llama3 "Hello"            # Use specific model
    ollamacode -f prompt.txt                # Read from file
    ollamacode --system "You are DevOps expert" "Setup nginx"

EOF
}

# Check if ollama is installed
check_ollama() {
    if ! command -v ollama &> /dev/null; then
        echo -e "${RED}✗ Error: ollama is not installed${NC}"
        echo -e "${YELLOW}Please install ollama from: https://ollama.ai${NC}"
        exit 1
    fi

    # Check if ollama is running
    if ! curl -s "${OLLAMA_HOST}/api/tags" &> /dev/null; then
        echo -e "${RED}✗ Error: ollama is not running${NC}"
        echo -e "${YELLOW}Start ollama with: ollama serve${NC}"
        exit 1
    fi
}

# List available models
list_models() {
    echo -e "${CYAN}${BOLD}Available Models:${NC}"
    ollama list | tail -n +2 | awk '{print "  " $1}' | sort
}

# Show configuration
show_config() {
    echo -e "${CYAN}${BOLD}Current Configuration:${NC}"
    echo -e "  Model:        ${GREEN}${MODEL}${NC}"
    echo -e "  Host:         ${OLLAMA_HOST}"
    echo -e "  Temperature:  ${TEMPERATURE}"
    echo -e "  Max Tokens:   ${MAX_TOKENS}"
    echo -e "  Config File:  ${CONFIG_FILE}"
    echo -e "  History:      ${HISTORY_FILE}"
}

# Send prompt to Ollama with tool execution support
send_prompt() {
    local user_prompt="$1"
    local show_stats="${2:-false}"

    echo -e "${BLUE}${BOLD}Thinking...${NC}"
    echo ""

    local start_time=$(date +%s)
    local total_tokens=0

    # Get system prompt with tool definitions
    local system_prompt=$(get_system_prompt)

    # Initialize messages array with system and user message
    local messages_json=$(jq -n \
        --arg sys "$system_prompt" \
        --arg user "$user_prompt" \
        '[{role: "system", content: $sys}, {role: "user", content: $user}]')

    # Tool execution loop
    local max_iterations=10
    local iteration=0

    while [[ $iteration -lt $max_iterations ]]; do
        iteration=$((iteration + 1))

        # Build JSON payload using chat API
        local temp_file=$(mktemp)
        jq -n \
            --arg model "${MODEL}" \
            --argjson messages "$messages_json" \
            --argjson temp "${TEMPERATURE}" \
            --argjson max_tokens "${MAX_TOKENS}" \
            '{
                model: $model,
                messages: $messages,
                stream: false,
                options: {
                    temperature: $temp,
                    num_predict: $max_tokens
                }
            }' > "$temp_file"

        # Send request to Ollama using chat API
        local response=$(curl -s -X POST "${OLLAMA_HOST}/api/chat" \
            -H "Content-Type: application/json" \
            --data-binary "@$temp_file")

        rm -f "$temp_file"

        # Extract response
        local ai_response=$(echo "${response}" | jq -r '.message.content')
        local eval_count=$(echo "${response}" | jq -r '.eval_count // 0')
        total_tokens=$((total_tokens + eval_count))

        # Check for tool calls
        local tool_calls_file=$(parse_tool_calls "$ai_response")
        local tool_count=$(count_tool_calls "$tool_calls_file")

        if [[ "$tool_count" -eq 0 ]]; then
            # No tool calls - print final response and exit loop
            local response_text=$(get_response_text "$ai_response")
            echo -e "${GREEN}${response_text}${NC}"
            echo ""
            [[ -f "$tool_calls_file" ]] && rm -f "$tool_calls_file"
            break
        fi

        # Print any text before tool calls
        local response_text=$(get_response_text "$ai_response")
        if [[ -n "$response_text" ]]; then
            echo -e "${GREEN}${response_text}${NC}"
            echo ""
        fi

        # Execute tool calls
        local tool_results=""
        for ((i=0; i<tool_count; i++)); do
            local tool_call=$(extract_tool_call "$tool_calls_file" "$i")
            local tool_name=$(parse_tool_name "$tool_call")

            echo -e "${CYAN}[Executing tool: ${tool_name}]${NC}"
            echo ""

            # Execute tool and capture output
            local tool_output=$(execute_tool "$tool_name" "$tool_call" 2>&1)
            local tool_exit_code=$?

            # Build tool result
            tool_results+="

=== Tool Result ${i} (${tool_name}) ===
Exit Code: ${tool_exit_code}
${tool_output}
========================

"
        done

        # Clean up temp file
        [[ -f "$tool_calls_file" ]] && rm -f "$tool_calls_file"

        # Add assistant response and tool results to messages
        local followup_message="Tool execution results:${tool_results}

Please analyze these results and provide your final answer, or execute more tools if needed."

        messages_json=$(echo "$messages_json" | jq \
            --arg assistant_msg "$ai_response" \
            --arg tool_msg "$followup_message" \
            '. + [{role: "assistant", content: $assistant_msg}, {role: "user", content: $tool_msg}]')

    done

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    # Show stats if requested
    if [[ "${show_stats}" == "true" ]]; then
        local tokens_per_sec=0
        if [[ ${duration} -gt 0 ]] && [[ ${total_tokens} -gt 0 ]]; then
            tokens_per_sec=$(awk "BEGIN {printf \"%.2f\", ${total_tokens}/${duration}}")
        fi

        echo -e "${MAGENTA}───────────────────────────────────────${NC}"
        echo -e "${MAGENTA}Stats: ${duration}s | ${total_tokens} tokens | ${tokens_per_sec} tok/s${NC}"
    fi

    # Save to history
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] ${user_prompt}" >> "${HISTORY_FILE}"
}

# Interactive mode
interactive_mode() {
    print_banner
    show_config
    echo ""

    local conversation_history=()

    while true; do
        echo -ne "${BOLD}${CYAN}You>${NC} "
        read -r user_input

        # Skip empty input
        [[ -z "${user_input}" ]] && continue

        # Handle commands
        case "${user_input}" in
            "exit"|"quit")
                echo -e "${GREEN}Goodbye!${NC}"
                exit 0
                ;;
            "help")
                usage
                continue
                ;;
            "models")
                list_models
                continue
                ;;
            "config")
                show_config
                continue
                ;;
            "clear")
                clear
                print_banner
                continue
                ;;
            "history")
                echo -e "${CYAN}${BOLD}Conversation History:${NC}"
                tail -20 "${HISTORY_FILE}"
                continue
                ;;
            use\ *)
                MODEL="${user_input#use }"
                echo -e "${GREEN}✓ Switched to model: ${MODEL}${NC}"
                # Cross-platform sed: macOS requires -i '' while Linux uses -i
                if [[ "$(uname)" == "Darwin" ]]; then
                    sed -i '' "s/^MODEL=.*/MODEL=${MODEL}/" "${CONFIG_FILE}"
                else
                    sed -i "s/^MODEL=.*/MODEL=${MODEL}/" "${CONFIG_FILE}"
                fi
                continue
                ;;
            temp\ *)
                TEMPERATURE="${user_input#temp }"
                echo -e "${GREEN}✓ Temperature set to: ${TEMPERATURE}${NC}"
                # Cross-platform sed: macOS requires -i '' while Linux uses -i
                if [[ "$(uname)" == "Darwin" ]]; then
                    sed -i '' "s/^TEMPERATURE=.*/TEMPERATURE=${TEMPERATURE}/" "${CONFIG_FILE}"
                else
                    sed -i "s/^TEMPERATURE=.*/TEMPERATURE=${TEMPERATURE}/" "${CONFIG_FILE}"
                fi
                continue
                ;;
            system\ *)
                SYSTEM_PROMPT="${user_input#system }"
                echo -e "${GREEN}✓ System prompt updated${NC}"
                continue
                ;;
            *)
                send_prompt "${user_input}" "true"
                ;;
        esac
    done
}

# Main function
main() {
    init_config
    load_config

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -m|--model)
                MODEL="$2"
                shift 2
                ;;
            -t|--temperature)
                TEMPERATURE="$2"
                shift 2
                ;;
            -s|--system)
                SYSTEM_PROMPT="$2"
                shift 2
                ;;
            -f|--file)
                PROMPT_FILE="$2"
                shift 2
                ;;
            -c|--config)
                show_config
                exit 0
                ;;
            -l|--list)
                list_models
                exit 0
                ;;
            -v|--version)
                echo "ollamaCode version ${VERSION}"
                exit 0
                ;;
            -h|--help)
                usage
                exit 0
                ;;
            *)
                DIRECT_PROMPT="$*"
                break
                ;;
        esac
    done

    check_ollama

    # Handle direct prompt or file input
    if [[ -n "${DIRECT_PROMPT:-}" ]]; then
        send_prompt "${DIRECT_PROMPT}" "false"
        exit 0
    elif [[ -n "${PROMPT_FILE:-}" ]]; then
        if [[ -f "${PROMPT_FILE}" ]]; then
            PROMPT_CONTENT=$(cat "${PROMPT_FILE}")
            send_prompt "${PROMPT_CONTENT}" "false"
            exit 0
        else
            echo -e "${RED}✗ File not found: ${PROMPT_FILE}${NC}"
            exit 1
        fi
    else
        # Start interactive mode
        interactive_mode
    fi
}

# Run main function
main "$@"
